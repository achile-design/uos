{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f258ce-3749-409e-8c36-ff91a4785a2b",
   "metadata": {},
   "source": [
    "#### 1. Business Understanding\n",
    "What business context is the data coming from? What insights would be valuable in that context, and what data would be required for that purporse?\n",
    "\n",
    "* Define objective or problem definition of the research in respect to company's business and available data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ba22d-78fc-4ff3-9e48-6ee377113963",
   "metadata": {},
   "source": [
    "#### a. Introduction\n",
    "\n",
    "* How I answered the above questions to create the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44366be1-145f-4d1e-bed1-308e4d421375",
   "metadata": {},
   "source": [
    "#### b. Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87d3fd8-59ce-444d-9b82-af816a793a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import libraries for numerical calculations, data and datetime processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# imported for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import folium\n",
    "\n",
    "#from folium.plugins import FastMarkerCluster\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins import HeatMap\n",
    "import branca.element as be\n",
    "import branca.colormap as cm\n",
    "\n",
    "# for processing text data, build model that generates vader score for intensity of customer ratings\n",
    "import re \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # vader score processing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# kmeans is used to cross validate vader output and fine tune the vader score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# to build huggy face ai model and make comparison with other librariew\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4edd08-1f88-4ebf-9e0d-c3aaea6289a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Dataframe\n",
    "csv_reviews_data = pd.read_csv('data/reviews.csv')\n",
    "csv_products_data = pd.read_csv('data/products.csv')\n",
    "csv_user_data = pd.read_csv('data/users.csv') \n",
    "\n",
    "# The 'lines=True' parameter is used here for json's file to mitigate ValueError: Trailing data\n",
    "json_reviewers_data = pd.read_json('data/jcpenney_reviewers.json', lines = True)\n",
    "json_products_data = pd.read_json('data/jcpenney_products.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051ba8-76a7-4a13-9d57-39ea9bfc5462",
   "metadata": {},
   "source": [
    "#### 2. Data understanding and preparation - \n",
    "Explore the data and show you understand its structure and relations, with the aid of appropriate visualisation techniques. Assess the data quality, which insights you would be able to answer from it, and what preparation the data would require. Add new data from another source if required to bring new insights to the data you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41778c31-e009-40d1-a6aa-5c55c69f95aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>63.588753</td>\n",
       "      <td>-154.493062</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code   latitude   longitude   state\n",
       "0   AK  63.588753 -154.493062  Alaska"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/states_data.csv').head(1) #added latitude and longitude data from another source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78c119-2103-40d4-9b38-6e26e2db3bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
